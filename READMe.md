# Subword Tokenizers From Scratch

[![Python 3.7+](https://img.shields.io/badge/python-3.7+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

An educational implementation of subword tokenization algorithms - Byte-Pair Encoding (BPE) and WordPiece tokenizers, built from scratch for learning and research purposes.

## Overview

This project provides clean and well-documented implementations of two fundamental subword tokenization algorithms:

- **Byte-Pair Encoding (BPE)**: The algorithm behind GPT and many transformer models
- **WordPiece**: Google's tokenization method used in BERT and related models